"""
This file contains a code for a generalized streamer.

The model's forward must take a windowed tensor of dimensions (n_batch, n_channels, window_size, num_windows)
and also output (n_batch, n_channels, window_size, num_windows). The first non-self positional argument
will be the windowed tensor, and the forward must have a keyword argument for `lookahead`.

Overlap-adding to a buffer is handled by the streamer. All completed audio is outputted immediately.

The streamer contains a lookahead window cache and queue, as well as an output buffer containing incomplete
audio. The last `lookahead_frames` frames of input audio should not have audio generated for them.

`hidden_refresh` is the number of input samples until the hidden state refreshes. This is to prevent out of
distribution hidden states from no voice for awhile.

For networks featuring a causal RNN, `recurrent` should be set true, and the following properties must hold
for `model.forward`:
 + Has kwarg `hx` that takes a torch.Tensor or a tuple of torch.Tensors, representing the hidden state
 + When kwarg `hx=None`, then the hidden state must be generated by the model.
 + Returns a tuple for size 2: (windowed_audio, new_hx)
"""

import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from src.data.dataproperties import DataProperties


class Streamer:
    def __init__(self,
                 model: nn.Module,
                 device: str,
                 hop_length: int = None,
                 hidden_refresh: int = 32_000,
                 window_length: int = None,
                 win_type: str = None,
                 lookahead_frames: int = None,
                 recurrent: bool = False):
        self.device = device
        self.model: nn.Module = model
        self.model.to(device)
        self.model.eval()
        # Assume hop_length is a multiple of window_length
        self.hop_length: int = hop_length or self.model.hop_length
        self.window_length: int = window_length or self.model.window_length
        self.output_buffer_length: int = self.window_length - self.hop_length
        self.hidden_refresh_frames: int = (hidden_refresh - self.window_length) // self.hop_length + 1
        self.lookahead_frames: int = self.model.lookahead_frames if lookahead_frames is None else lookahead_frames
        self.win_type: str = win_type or self.model.win_type
        self.recurrent: bool = recurrent

        # Timing variables
        self._last_processed_samples: int = 0
        self._total_processed_samples: int = 0
        self._last_processed_time: float = 0
        self._total_processed_time: float = 0

        self._reset_buffers()

    @property
    def real_time_factor(self) -> float:
        """Gets the real time factor of all current processed audio"""
        return self._total_processed_time / (self._total_processed_samples / DataProperties.get('sample_rate'))

    def _refresh_hidden_state(self, input_frames: torch.Tensor) -> None:
        num_frames = input_frames.shape[1]
        self._frames_until_refresh -= num_frames
        if self._frames_until_refresh <= 0:
            self.hx = None

    def _reset_buffers(self):
        """Clears out buffers"""
        self.lookahead_buffer: torch.Tensor = torch.zeros(1, 0, self.window_length, device=self.device)
        self.output_buffer: torch.Tensor = torch.zeros(1, 0, device=self.device)
        # next_pad contains the first `self.hop_length` samples of a window, where half of it is in
        # one `feed` chunk, and the other is in the next
        self.next_pad: torch.Tensor = None
        self.hx = None
        self._frames_until_refresh = self.hidden_refresh_frames

    def _get_windows(self, x: torch.Tensor) -> torch.Tensor:
        """
        Gets (possibly) overlapping windows of `x`.
        If windows overlap, the last `self.hop_size` frames will be appended to the start of `x`.

        (1, 1, length) -> (1, num_windows, window_length)
        """
        """
        If we are using overlapping windows, then take the last `self.hop_length`
        of the last window in the `self.lookahead_buffer`
        """
        if self.win_type in ['hann', 'triangular']:
            if self.next_pad is not None:
                x = torch.cat([self.next_pad, x], dim=-1)
            self.next_pad = x[:, :, -self.hop_length:]

        windowed_audio = F.unfold(
           x.unsqueeze(2),
           (1, self.window_length),
           stride=self.hop_length
        ).permute(0, 2, 1)

        if self.lookahead_frames > 0:
            windowed_audio = torch.cat(
                [self.lookahead_buffer, windowed_audio], dim=1
            )  # (1, lookahead_buffer_windows + input_windows, window_length)
            self.lookahead_buffer = windowed_audio[:, -self.lookahead_frames:, :]
        return windowed_audio

    @staticmethod
    def _get_win_func(win_type: str):
        if win_type == 'rectangular':
            return lambda m: torch.ones(m)
        elif win_type == 'hann':
            return lambda m: torch.hann_window(m)
        elif win_type == 'triangular':
            return lambda m: torch.as_tensor(np.bartlett(m)).float()
        else:
            raise ValueError("Invalid window type")

    def _overlap_add_buffer(self, frames: torch.Tensor) -> None:
        """
        Takes `frames`, applies a windowed overlap-add to them.
        Adds the previous overlap-add buffer to the start of the folded
        signal, and assigns signal to `self.output_buffer`

        :param frames: (n_batch, n_frames, window_length)
        """
        # Length of output frames may be longer than self.window_length.
        # Extra frames will be truncated.
        assert frames.shape[-1] >= self.window_length
        frames = frames[:, :, :self.window_length]

        if self.win_type in ['triangular', 'hann']:
            win = self._get_win_func(self.win_type)(self.window_length).to(frames).reshape(1, 1, -1)
            # Instead of padding the window, we chop the frames
            # apply window
            frames = frames * win
        elif self.win_type == 'rectangular':
            pass  # for consistency with non-streamer versions
        else:
            raise ValueError(f'Invalid windows type: {self.win_type}')
        n_frames = frames.shape[1]
        buffer = torch.zeros(1, 1, (n_frames - 1) * self.hop_length + self.window_length, device=self.device)
        start_buffer = 0
        for frame_idx in range(n_frames):
            buffer[..., start_buffer:start_buffer+self.window_length] += frames[:, frame_idx, :]
            start_buffer += self.hop_length
        buffer[..., :self.output_buffer.shape[-1]] += self.output_buffer
        self.output_buffer = buffer

    def _get_output_audio(self):
        """
        Returns the complete output audio from `self.output_buffer`,
        and truncates `self.output_buffer` to only contain `output_buffer_length`
        samples.
        """
        buffer_length = self.output_buffer.shape[-1]

        return_audio, self.output_buffer = torch.split(
            self.output_buffer,
            split_size_or_sections=[buffer_length - self.output_buffer_length,
                                    self.output_buffer_length],
            dim=-1
        )
        return return_audio

    def _log_time(self, output_audio: torch.Tensor, time_begin: float) -> None:
        elapsed = time.time() - time_begin
        self._last_processed_samples = output_audio.shape[-1]
        self._last_processed_time = elapsed
        self._total_processed_samples += self._last_processed_samples
        self._total_processed_time += self._last_processed_time

    @torch.no_grad()
    def feed(self, audio: torch.Tensor, **kwargs) -> torch.Tensor:
        """
        :param audio: FloatTensor of shape (1, 1, length). `length` should be
                      a multiple of `self.hop_length`.
        """
        time_begin = time.time()
        windowed_audio = self._get_windows(audio)  # (1, input_windows, window_length)

        if self.recurrent:
            output_frames, self.hx = self.model(windowed_audio, hx=self.hx, **kwargs)
        else:
            output_frames = self.model(windowed_audio, **kwargs)
        # output_frames is size (1, output_windows, window_length)

        self._overlap_add_buffer(output_frames)
        output_audio = self._get_output_audio()
        self._log_time(output_audio, time_begin)
        return output_audio

    @torch.no_grad()
    def flush(self, **kwargs):
        """
        This gets the remaining audio out of the buffers, processes it, and returns it.

        Does not log time.
        """
        if self.lookahead_frames:
            model_input = torch.cat(
                [self.lookahead_buffer, torch.zeros(1, self.lookahead_frames, self.window_length, device=self.device)],
                dim=1
            )  # (1, lookahead_buffer_windows + self.lookahead_frames, window_length)
            if self.recurrent:
                output_frames, self.hx = self.model(model_input, hx=self.hx, **kwargs)
            else:
                output_frames = self.model(model_input, **kwargs)  # (1, output_windows, window_length)
            self._overlap_add_buffer(output_frames)
        output_audio = self.output_buffer
        self._reset_buffers()
        return output_audio
